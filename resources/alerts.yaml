apiVersion: v1
kind: ConfigMap
metadata:
  name: kapacitor-alerts
  namespace: kube-system
data:
  high_cpu.tick: |
    var period = 5m
    var every = 1m
    var warnRate = 75
    var warnReset = 50
    var critRate = 90
    var critReset = 75

    var usage_rate = stream
        |from()
            .measurement('cpu/usage_rate')
            .groupBy('nodename')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var cpu_total = stream
        |from()
            .measurement('cpu/node_capacity')
            .groupBy('nodename')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var percent_used = usage_rate
        |join(cpu_total)
            .as('usage_rate', 'total')
            .tolerance(30s)
            .streamName('percent_used')
        |eval(lambda: (float("usage_rate.value") * 100.0) / float("total.value"))
            .as('percent_usage')
        |mean('percent_usage')
            .as('avg_percent_used')

    var trigger = percent_used
        |alert()
            .message('{{ .Level}} / Node {{ index .Tags "nodename" }} has high cpu usage: {{ index .Fields "avg_percent_used" }}%')
            .warn(lambda: "avg_percent_used" > warnRate)
            .warnReset(lambda: "avg_percent_used" < warnReset)
            .crit(lambda: "avg_percent_used" > critRate)
            .critReset(lambda: "avg_percent_used" < critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    <p>Usage: {{ index .Fields "avg_percent_used"  | printf "%0.2f" }}%</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/high_cpu.log')
            .mode(0644)

  high_memory.tick: |
    var period = 5m
    var every = 1m
    var warnRate = 80
    var warnReset = 70
    var critRate = 90
    var critReset = 80

    var usage = stream
        |from()
            .measurement('memory/working_set')
            .groupBy('nodename')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var memory_total = stream
        |from()
            .measurement('memory/node_allocatable')
            .groupBy('nodename')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var percent_used = usage
        |join(memory_total)
            .as('usage', 'total')
            .tolerance(30s)
            .streamName('percent_used')
        |eval(lambda: (float("usage.value") * 100.0) / float("total.value"))
            .as('percent_usage')
        |mean('percent_usage')
            .as('avg_percent_used')

    var trigger = percent_used
        |alert()
            .message('{{ .Level}} / Node {{ index .Tags "nodename" }} has high memory usage: {{ index .Fields "avg_percent_used" }}%')
            .warn(lambda: "avg_percent_used" > warnRate)
            .warnReset(lambda: "avg_percent_used" < warnReset)
            .crit(lambda: "avg_percent_used" > critRate)
            .critReset(lambda: "avg_percent_used" < critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    <p>Usage: {{ index .Fields "avg_percent_used"  | printf "%0.2f" }}%</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/high_memory.log')
            .mode(0644)

  filesystem.tick: |
    var period = 1m
    var every = 1m
    var warnRate = 80
    var warnReset = 70
    var critRate = 90
    var critReset = 80

    var usage = stream
        |from()
            .measurement('filesystem/usage')
            .groupBy('nodename', 'resource_id')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var total = stream
        |from()
            .measurement('filesystem/limit')
            .groupBy('nodename', 'resource_id')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var percent_used = usage
        |join(total)
            .as('usage', 'total')
            .tolerance(30s)
            .streamName('percent_used')
        |eval(lambda: (float("usage.value") * 100.0) / float("total.value"))
            .as('percent_used')

    var trigger = percent_used
        |alert()
            .message('{{ .Level}} / Node {{ index .Tags "nodename" }} has low free space on {{ index .Tags "resource_id" }}')
            .warn(lambda: "percent_used" > warnRate)
            .warnReset(lambda: "percent_used" < warnReset)
            .crit(lambda: "percent_used" > critRate)
            .critReset(lambda: "percent_used" < critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    <p>Resource: {{ index .Tags "resource_id" }}</p>
    <p>Usage: {{ index .Fields "percent_used"  | printf "%0.2f" }}%</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/filesystem.log')
            .mode(0644)

    var warnInodes = 90
    var warnInodesReset = 80
    var critInodes = 95
    var critInodesReset = 90

    var inodes_free = stream
        |from()
            .measurement('filesystem/inodes_free')
            .groupBy('nodename', 'resource_id')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var inodes_total = stream
        |from()
            .measurement('filesystem/inodes')
            .groupBy('nodename', 'resource_id')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)

    var percent_used_inodes = inodes_free
        |join(inodes_total)
            .as('free', 'total')
            .tolerance(30s)
        |eval(lambda: (100.0 - (float("free.value") * 100.0) / float("total.value")))
            .as('percent_used_inodes')

    var trigger_inodes = percent_used_inodes
        |alert()
            .message('{{ .Level}} / Node {{ index .Tags "nodename" }} has low free inodes on {{ index .Tags "resource_id" }}')
            .warn(lambda: "percent_used_inodes" > warnInodes)
            .warnReset(lambda: "percent_used_inodes" < warnInodesReset)
            .crit(lambda: "percent_used_inodes" > critInodes)
            .critReset(lambda: "percent_used_inodes" < critInodesReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    <p>Resource: {{ index .Tags "resource_id" }}</p>
    <p>Usage: {{ index .Fields "percent_used_inodes"  | printf "%0.2f" }}%</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/inodes.log')
            .mode(0644)

  uptime.tick: |
    var period = 1m
    var every = 1m
    var warn = 300 // seconds
    var warnReset = 600 // seconds

    var node_down = stream
        |from()
            .measurement('uptime')
            .groupBy('*')
            .where(lambda: "type" == 'node')
        |deadman(0.0, 5m)
            .message('Node {{ index .Tags "nodename" }} is down')
            .stateChangesOnly()
            .email()
            .log('/var/lib/kapacitor/logs/node_down.log')
            .mode(0644)

    var uptime = stream
        |from()
            .measurement('uptime')
            .groupBy('nodename')
            .where(lambda: "type" == 'node')
        |window()
            .period(period)
            .every(every)
        |eval(lambda: ceil(float("value") / 1000.0))
            .as('uptime')

    var trigger = uptime
        |alert()
            .message('{{ .Level }} / Node {{ index .Tags "nodename" }} was rebooted')
            .warn(lambda: "uptime" < warn)
            .warnReset(lambda: "uptime" > warnReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    <p>Uptime: {{ index .Fields "uptime" }} sec</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/uptime.log')
            .mode(0644)

  etcd.tick: |
    var period = 1m
    var every = 1m
    var critReset = lambda: "gauge" == 1
    var data_etcd_up = stream
        |from()
            .measurement('satellite_etcd_up')
            .groupBy('nodename')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', 0)

    var trigger_etcd_up = data_etcd_up
        |alert()
            .message('{{ .Level }} / etcd: instance is down: {{ index .Tags "nodename" }}')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly(1h)
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/etcd_down.log')
            .mode(0644)

    var data_etcd_health = stream
        |from()
            .measurement('satellite_etcd_health')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var etcd_health_deadman = data_etcd_health
        |deadman(0.0, 5m)
            .message('etcd cluster is {{ if eq .Level "OK" }}alive{{ else }}unhealthy{{ end }}')
            .email()
            .log('/var/lib/kapacitor/logs/etcd_health.log')
            .mode(0644)

    var trigger_etcd_health = data_etcd_health
        |alert()
            .message('{{ .Level }} / etcd: cluster is unhealthy')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly(1h)
            .details('''
    <b>{{ .Message }}</b>
    Level: {{ .Level }}
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/etcd_health.log')
            .mode(0644)

  etcd_latency_batch.tick: |
    var period = 5m
    var every = 1m
    var data_etcd_latency = batch
        |query('''SELECT (DERIVATIVE(count,1m) * 0.95) AS count, DERIVATIVE("0.512",1m) AS v512, DERIVATIVE("1.024",1m) AS v1024 FROM "k8s"."default"."etcd_rafthttp_message_sent_latency_seconds" WHERE "msgType" = 'MsgHeartbeat' AND "sendingType" = 'message' ''')
            .period(period)
            .every(every)
            .groupBy('remoteID')

    var count = data_etcd_latency
        |mean('count')
    var v512 = data_etcd_latency
        |mean('v512')
    var v1024 = data_etcd_latency
        |mean('v1024')

    var trigger_etcd_latency = count
        |join(v512,v1024)
            .as('count', 'v512', 'v1024')
            .tolerance(10s)

    trigger_etcd_latency
        |alert()
            .message('{{ .Level }} / etcd: High latency between leader and follower {{ index .Tags "followerName" }}')
            .warn(lambda: "count.mean" > "v512.mean")
            .crit(lambda: "count.mean" > "v1024.mean")
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>etcd instance: {{ index .Tags "followerName" }}</p>
    <p>Latency greater than: {{ if eq .Level "WARNING" }}512{{ else }}1024{{ end }}ms</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/etcd_latency.log')
            .mode(0644)

  networking_params.tick: |
    var period = 5m
    var every = 1m
    var critReset = lambda: "gauge" == 1
    var data_br_netfilter = stream
        |from()
            .measurement('satellite_sysctl_br_netfilter')
            .groupBy('nodename')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var deadman_br_netfilter = data_br_netfilter
        |deadman(0.0, 5m)
            .message('br_netfilter module is not loaded on node {{ index .Tags "nodename" }}')
            .email()

    var trigger_br_netfilter = data_br_netfilter
        |alert()
            .message('{{ .Level }} / Networking: bridge netfilter is disabled on node: {{ index .Tags "nodename" }}')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/br_netfilter.log')
            .mode(0644)

    var data_ipv4_forwarding = stream
        |from()
            .measurement('satellite_sysctl_ipv4_forwarding')
            .groupBy('nodename')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var trigger_ipv4_forwarding = data_ipv4_forwarding
        |alert()
            .message('{{ .Level }} / Networking: IPv4 Forwarding is disabled on node: {{ index .Tags "nodename" }}')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/ipv4_forwarding.log')
            .mode(0644)

  docker.tick: |
    var period = 5m
    var every = 1m
    var critReset = lambda: "gauge" == 1
    var data_docker = stream
        |from()
            .measurement('satellite_docker_health')
            .groupBy('nodename')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var trigger_docker = data_docker
        |alert()
            .message('{{ .Level }} / Docker daemon is down on host: {{ index .Tags "nodename" }}')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/docker_health.log')
            .mode(0644)

  influxdb_health_batch.tick: |
    var period = 5m
    var every = 1m
    var data_influxdb_health = batch
        |query('''SELECT * FROM "k8s"."default"."cpu/usage"''')
            .period(period)
            .every(every)

    var deadman_influxdb_health = data_influxdb_health
        |deadman(0.0, 5m)
            .message('InfluxDB is down or no connection between Kapacitor and InfluxDB')
            .email()
            .log('/var/lib/kapacitor/logs/influxdb_health.log')
            .mode(0644)

  kubernetes_node.tick: |
    var period = 5m
    var every = 1m
    var critReset = lambda: "gauge" == 1
    var data_kubernetes = stream
        |from()
            .measurement('satellite_k8s_node_ready')
            .groupBy('node')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)

    var trigger_kubernetes = data_kubernetes
        |alert()
            .message('{{ .Level }} / Kubernetes node is not ready: {{ index .Tags "node" }}')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "node" }}</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/kubernetes_health.log')
            .mode(0644)

  systemd: |
    var period = 5m
    var every = 1m
    var critReset = lambda: "gauge" == 1
    var data_systemd_health = stream
        |from()
            .measurement('satellite_systemd_health')
            .groupBy('nodename')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)
    var trigger_systemd_health = data_systemd_health
        |alert()
            .message('{{ .Level }} / Systemd on host {{ index .Tags "nodename" }} is degraded')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/systemd_health.log')
            .mode(0644)
    var data_systemd_units = stream
        |from()
            .measurement('satellite_systemd_unit_health')
            .groupBy('nodename', 'unit_name')
        |window()
            .period(period)
            .every(every)
            .align()
        |default()
            .field('gauge', -1)
    var trigger_systemd_units = data_systemd_units
        |alert()
            .message('{{ .Level }} / Systemd unit {{ index .Tags "unit_name" }} is degraded')
            .crit(lambda: "gauge" == 0)
            .critReset(critReset)
            .stateChangesOnly()
            .details('''
    <b>{{ .Message }}</b>
    <p>Level: {{ .Level }}</p>
    <p>Nodename: {{ index .Tags "nodename" }}</p>
    ''')
            .email()
            .log('/var/lib/kapacitor/logs/systemd_units_health.log')
            .mode(0644)
